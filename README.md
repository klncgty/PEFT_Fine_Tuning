EÄŸitilmiÅŸ bÃ¼yÃ¼k dil modellerinin tÃ¼m parametreleriyle Fine-tuning yapÄ±lmasÄ± bÃ¼yÃ¼k maliyettir. 
PEFT ile modelin tÃ¼m parametreleri yerine yalnÄ±zca Ã§ok az sayÄ±da model parametresine ince ayar yaparak olmasÄ±nÄ± istediÄŸimiz davranÄ±ÅŸ ÅŸeklini modele dikte edebiliriz.

Parameter efficient fine-tuning (PEFT), modellerin boyutunu kÃ¼Ã§Ã¼ltmeyi amaÃ§layan ve daha az gÃ¼Ã§lÃ¼ GPU'larda hesaplamalar yapmayÄ± mÃ¼mkÃ¼n kÄ±lan bir yÃ¶ntemdir. 
LoRa, PEFT'de LLM'lerin boyutunu kÃ¼Ã§Ã¼ltmek iÃ§in kullanÄ±lan bir yÃ¶ntemdir.

PEFT, LoRA ve P-Tunign gibi adaptÃ¶rlerle entegre edilmiÅŸtir. 
![Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2024-05-06 075350](https://github.com/klncgty/PEFT_Fine_Tuning/assets/107580070/d31d3ae9-212b-4da0-9da1-350e4334c9cb)

Bu repoda Ã§alÄ±ÅŸÄ±lan 2 adet PEFT metodu var. Birincisi reparameterization ( LoRA), ikincisi additive(p-tuning).

LoRA genelde eÄŸitilebilir parametrelerin sayÄ±sÄ±nÄ± Ã¶nemli Ã¶lÃ§Ã¼de azalttÄ±ÄŸÄ±ndan tek bir GPU ile modeli fine tune etmek mÃ¼mkÃ¼n. 
![Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2024-05-06 081439](https://github.com/klncgty/PEFT_Fine_Tuning/assets/107580070/203b4667-e0b4-4163-aba7-739269bbe406)

Bu kadar az parametre deÄŸiÅŸikliÄŸi ile bÃ¼yÃ¼k iÅŸlerin altÄ±ndan kalkmasÄ± LoRA'yÄ± Ã§ok cazip kÄ±lÄ±yor. Bir de devasa GPU'lar gerekmiyor. Ä°ÅŸte bu Ã§ok Ã§ok iyi.

Prompt tuning'e gelirsek, herhangi bir parametre deÄŸiÅŸikliÄŸi yapmadan modeli fine tune etmeye yarar. Peki nasÄ±l oluyor ince ayar? Modele eklenen yeni katmanlar eÄŸtiilmiÅŸ oluyor. Bu yÃ¼zden bu tekniÄŸin adÄ±
Additive. Yani prompt ile alakalÄ± layerlar ekleyip bunlarÄ± eÄŸitiyoruz.
![maxresdefault](https://github.com/klncgty/PEFT_Fine_Tuning/assets/107580070/fe1470ba-7067-4a11-b6a4-8fff9b7fc7f4)

GÃ¶sterildiÄŸi gibi, prompt tuning, girdi promtÄ±na Ã¶ÄŸrenilebilir bir parametre ekleyerek, hedef gÃ¶rev iÃ§in kaybÄ± minimize etmek amacÄ±yla optimize edilir. Bu, modelin belirli bir gÃ¶rev veya veri kÃ¼mesine uyarlanmasÄ±na izin verir, aÄŸÄ±rlÄ±klarÄ±nÄ± gÃ¼ncellemek zorunda kalmaz. Prompt tuning, geleneksel fine-tuning'den daha maliyetsiz ve verimli olabilir, Ã§Ã¼nkÃ¼ sadece promt parametresini gÃ¼ncellemek gerekir.

Referans ve ek dÃ¶kÃ¼man iÃ§in ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡


```
@Misc{peft,
  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}
``` 

NasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±na dair detaylÄ± aÃ§Ä±klama ve araÅŸtÄ±rma-makaleler iÃ§eren link:
https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft
