{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YxcgT8r5sr8M"
      },
      "outputs": [],
      "source": [
        "# LoRA lora ile fine tune ederken\n",
        "## 50x50 bir matrisimiz varsa 2500 parametremiz var.\n",
        "### ve biz 200 parametreyi değiştirmiş oluyoruz.\n",
        "#### bu da az parametre değişikliğiyle çok büyük değişiklikler elde etmektir."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q peft==0.8.2\n",
        "!pip install -q datasets==2.16.1"
      ],
      "metadata": {
        "id": "1geZrp6vttsB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers kütühanesini kullanarak model ve tokenize import etme işlemi.\n",
        "# Bir dil modeli mimarisi vermeden, bunu model kendi kendine anlaması için AutoNodelForCasualLM modeli.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "model_name = \"bigscience/bloomz-560m\"\n",
        "\n",
        "# bloom çok küçük ve akıllı bir model. Modelimiz foundation_model.\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NgCbLBauBXP",
        "outputId": "227acf0a-4013-4103-f61c-2c46034c3d7f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# elimizdeki modeli fine_tuning etmeden deneyelim. Bizim için\n",
        "# bu görevi yapacak bir fonksiyon\n",
        "\n",
        "def get_output(model, inputs, max_new_tokens=100):\n",
        "  outputs = model.generate(\n",
        "      input_ids = inputs[\"input_ids\"],\n",
        "      attention_mask= inputs[\"attention_mask\"],\n",
        "      max_new_tokens = max_new_tokens,\n",
        "      repetition_penalty = 1.5, # tekrarlardan kaçınmak için\n",
        "      early_stopping = True, # max uzunluğa ulaştığı zaman model ürteimi durdursun\n",
        "      eos_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  return outputs"
      ],
      "metadata": {
        "id": "ZxJRD_Tfvsp2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = tokenizer(\"ı want you to be a motivational coach.   \",return_tensors=\"pt\")\n",
        "foundational_outputs_sentence = get_output(foundation_model, input_sentence,max_new_tokens=100)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT640l0twuGn",
        "outputId": "73321dbf-ffa4-4c73-fb4b-823139b5f222"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ı want you to be a motivational coach.    I know that this is not your job.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Başarısız. şimdi eğitim için hazırlıklar."
      ],
      "metadata": {
        "id": "XzDmNz_LyXov"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## huggingface de chatgpt promptlarını içeren veri seti\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = \"fka/awesome-chatgpt-prompts\"\n",
        "\n",
        "data = load_dataset(dataset)\n",
        "data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_w8ZoR3yWgv",
        "outputId": "2c2ec049-9793-461e-9468-40008d5cc6ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['act', 'prompt'],\n",
              "        num_rows: 153\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)"
      ],
      "metadata": {
        "id": "0Wbk6ouj1Oyo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizerdan geçirdiğim prompt sütunundan sonra input_ids ve attention_mask sütunları oluştu.\n",
        "\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es87PMZQ1ZG1",
        "outputId": "86adba40-77a2-4e66-858d-dbf5d41790e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['act', 'prompt', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 153\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample = data[\"train\"].select(range(50))   # bir liste üzerinde çalışlıldığı için select uygun.\n",
        "train_sample = train_sample.remove_columns('act')\n",
        "\n",
        "display(train_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "ltLzz2glzdCx",
        "outputId": "1745e38b-237b-4bc9-e455-8ccac11187f5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_sample[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1d5VHLQ0qLq",
        "outputId": "8f6930a8-d601-4829-e8c6-12a6f9acdc09"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': ['I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd'], 'input_ids': [[44, 4026, 1152, 427, 1769, 661, 267, 104105, 28434, 17, 473, 2152, 4105, 49123, 530, 1152, 2152, 57502, 1002, 3595, 368, 28434, 3403, 6460, 17, 473, 4026, 1152, 427, 3804, 57502, 1002, 368, 28434, 10014, 14652, 2592, 19826, 4400, 10973, 15, 530, 16915, 4384, 17, 727, 1130, 11602, 184637, 17, 727, 1130, 4105, 49123, 35262, 473, 32247, 1152, 427, 727, 1427, 17, 3262, 707, 3423, 427, 13485, 1152, 7747, 361, 170205, 15, 707, 2152, 727, 1427, 1331, 55385, 5484, 14652, 6291, 999, 117805, 731, 29726, 1119, 96, 17, 2670, 3968, 9361, 632, 269, 42512]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine Tuning\n",
        "\n",
        "import peft\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=4, # daha büyük r eğitim için daha büyük train demek\n",
        "    lora_alpha=1, #ağırlık matrisi için. genelde 1 olarak kalır\n",
        "    target_modules=[\"query_key_value\"], # genellikle metin anlamını belirleyen veya dil bilgisi yapılarını öğrenen dönüşümsel katmanları hedefleyebilirsiniz. Bu katmanlar, genellikle dil modelinin temelini oluşturur ve büyük ağırlık matrislerine sahiptir. LoRA kullanarak, bu tür katmanların boyutunu azaltarak modelin daha hafif ve hızlı çalışır.\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"lora_only\",\n",
        "    task_type=\"CASUAL_LM\"\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "En önemli parametre r dır. Ne kadar parametrenin bu modelde eğitileceğine karar verir.\n",
        "çok büyük r yaparsak mmodel girdi ve çıktı arasındaki kompleks ilişkiyii daha iyi kavrar.\n",
        "task_type şudur. modelin amacı ne? bu modelin amacı text generation..\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "wjJtMPie1va7",
        "outputId": "19bf1b13-648f-44e3-efb6-28b67bc5168f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEn önemli parametre r dır. Ne kadar parametrenin bu modelde eğitileceğine karar verir.\\nçok büyük r yaparsak mmodel girdi ve çıktı arasındaki kompleks ilişkiyii daha iyi kavrar.\\ntask_type şudur. modelin amacı ne? bu modelin amacı text generation.. \\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## PEFT model\n",
        "\n",
        "peft_model = get_peft_model(foundation_model, lora_config)\n",
        "print(peft_model.print_trainable_parameters())\n",
        "\n",
        "# modelin eğitilecek parametre sayısı tümüyle karşılaştığımızda ne kadar küçük ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St1s-quJ3qlB",
        "outputId": "42afb582-4625-44c9-ef4a-12a91299c987"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 393,216 || all params: 559,607,808 || trainable%: 0.07026635339584111\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "working_dir='./'\n",
        "\n",
        "# burada bir çıktı dizini ayarlanmış oldu\n",
        "\n",
        "output_directory = os.path.join(working_dir, 'peft_lab_outpus')"
      ],
      "metadata": {
        "id": "EnNg8e5-37-p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training Args'da epoch sayısını ve çıktı dizininini ayrıca learning rate 'i belirleriz\n",
        "\n",
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = output_directory,\n",
        "    auto_find_batch_size = True, #  veriyle en uyumlu olacak batch büyüklüğünü ayarlar.\n",
        "    learning_rate = 3e-2,\n",
        "    num_train_epochs = 2,\n",
        "    use_cpu = True\n",
        ")"
      ],
      "metadata": {
        "id": "E75fTBX54l3b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Artık modeli eğtimek için tüm argumanlarımızı oluşşturduk.\n",
        "\"\"\"\n",
        "* PEFT Model\n",
        "* Training_args\n",
        "* Dataset\n",
        "* datacollator  model eğitimi için veri setinden alınan örnekleri hazırlamak ve düzenlemek için kullanılır\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#  5-10 dakika sürebilir.\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset = train_sample,\n",
        "    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "F8_bpHhw5PYP",
        "outputId": "48cdbb80-6054-42e6-ebfd-c282e48797d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14/14 06:05, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=14, training_loss=2.725315366472517, metrics={'train_runtime': 396.26, 'train_samples_per_second': 0.252, 'train_steps_per_second': 0.035, 'total_flos': 20073085894656.0, 'train_loss': 2.725315366472517, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# modeli kaydetme\n",
        "\n",
        "peft_model_path = os.path.join(output_directory,f\"lora_model_yeni\")\n",
        "\n",
        "trainer.model.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "eCtRHaHo8qxE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modeli yükleme\n",
        "\n",
        "loaded_model = PeftModel.from_pretrained(foundation_model,peft_model_path,is_trainable=False)"
      ],
      "metadata": {
        "id": "1m4ApN9y9Bn2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = tokenizer(\"ı want you to be a motivational coach.\", return_tensors=\"pt\")\n",
        "foundational_outputs_sentence = get_output(loaded_model,input_sentence, max_new_tokens=50)\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "QRqP7LLH9N5-",
        "outputId": "cbc64a35-039c-43f4-95c9-462a3d408c9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ı want you to be a motivational coach. I will provide the information about your goals and lifestyle style, as well as:  My first request is \"I need help improving my life\" \"I\\'ve been struggling with depression lately so it\\'s hard for me not being able focus on anything else than workouts!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###    eğitilmiş ve eğitilmemiş model arasındaki fark çok büyük."
      ],
      "metadata": {
        "id": "TkEB9WPF-SO0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}